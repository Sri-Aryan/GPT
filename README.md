# **Bigram Language Model â€“ GPT Implementation** ğŸš€  

A **Transformer-based Bigram Language Model** built using **PyTorch**. This project implements **self-attention mechanisms** and **embedding layers** for text generation. The model is trained on **Shakespeare's dataset** and generates text using a **causal transformer** approach.  

---

## ğŸ“Œ **Features**  
âœ… **Character-Level Text Generation** using a **bigram-based** self-attention model.  
âœ… **Positional Encoding** for better contextual understanding.  
âœ… **Masked Attention** to prevent information leakage during training.  
âœ… **Trained on Shakespeareâ€™s dataset**, enabling natural text generation.  
âœ… **Optimization using AdamW** for efficient training.  

---

## ğŸ› ï¸ **Tech Stack**  
- **Framework**: PyTorch  
- **Training Data**: Tiny Shakespeare dataset  
- **Model Architecture**: Transformer-based bigram model  
- **Optimization Algorithm**: AdamW  
- **Hardware**: CUDA-enabled GPU (if available)  

---

## ğŸš€ **How It Works**  

### **1ï¸âƒ£ Data Preprocessing**  
- Download **Tiny Shakespeare Dataset**.  
- Encode characters into numerical tokens for processing.  
- Create a **lookup table** (stoi & itos) for character mapping.  

### **2ï¸âƒ£ Model Architecture**  
- **Embedding Layers**: Convert input tokens into dense representations.  
- **Positional Encoding**: Adds positional context to embeddings.  
- **Masked Self-Attention**: Prevents future token access during training.  
- **Feedforward Network**: Processes token embeddings to generate logits.  

### **3ï¸âƒ£ Training Process**  
- Splits dataset into **train & validation** sets.  
- Uses **Cross Entropy Loss** for optimization.  
- Trained for **10,000 steps** using **AdamW optimizer**.  

### **4ï¸âƒ£ Text Generation**  
- Starts with a random seed token.  
- Uses **Softmax sampling** to predict next characters.  
- Generates text by iteratively predicting the next token.  

---

## ğŸ”„ **Model Training Flow**  
1ï¸âƒ£ **Data Encoding** â€“ Convert text into token sequences.  
2ï¸âƒ£ **Batch Generation** â€“ Extract batches for training.  
3ï¸âƒ£ **Embedding & Attention Mechanism** â€“ Generate meaningful representations.  
4ï¸âƒ£ **Training Loop** â€“ Backpropagation and weight updates.  
5ï¸âƒ£ **Text Generation** â€“ Predict the next token using trained weights.  

---

## ğŸ“ˆ **Performance & Results**  
- Generates **realistic Shakespearean-style text** after training.  
- Uses **self-attention** for context-aware text generation.  
- **Future Improvements**: Multi-head attention, larger datasets, hyperparameter tuning.  

